# VERSION               0.1
#

FROM vardhanv/cassandra


#
# Most of these variables should be moved to config files
# SPARK_DIST_CLASSPATH should be setup using $(hadoop classpath) and put into 
# spark_env.sh. Some of the code we need is as below for future reference.... 
#
# echo 'export  SPARK_DIST_CLASSPATH=$(hadoop classpath)' >> spark-env.sh
#. $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh
#. $SPARK_HOME/conf/spark-env.sh
#


ENV  JAVA_HOME=/usr/lib/jvm/java-8-oracle
ENV  HADOOP_PREFIX=/usr/local/hadoop
ENV  HADOOP_COMMON_HOME=/usr/local/hadoop 
ENV  HADOOP_HOME=/usr/local/hadoop 
ENV  HADOOP_HDFS_HOME=/usr/local/hadoop 
ENV  HADOOP_MAPRED_HOME=/usr/local/hadoop 
ENV  HADOOP_YARN_HOME=/usr/local/hadoop 
ENV  HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop 
ENV  YARN_CONF_DIR=$HADOOP_PREFIX/etc/hadoop 
ENV  SPARK_HOME=/usr/local/spark 
ENV  PATH=$PATH:$JAVA_HOME/bin:$HADOOP_PREFIX/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV  SPARK_DIST_CLASSPATH=/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar


#installing hadoop
RUN \
    curl http://www.us.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz | tar -xvz -C /usr/local/ && \ 
    cd /usr/local && ln -s ./hadoop-2.7.1 hadoop && \ 
    mkdir $HADOOP_PREFIX/input && \
    cp $HADOOP_PREFIX/etc/hadoop/*.xml $HADOOP_PREFIX/input


# Installing spark

RUN \ 
    curl http://www.us.apache.org/dist//spark/spark-1.4.1/spark-1.4.1-bin-without-hadoop.tgz | tar -xvz -C /usr/local 




# Be carefull, the SPARK_MASTER_IP needs to be reset for a slave node, other it will have the wrong ip address
# the below only works if the master and slave are executing from the same node
RUN \ 
    cd /usr/local && ln -s spark-1.4.1-bin-without-hadoop spark &&\ 
    cd /usr/local/spark/conf/ && cp spark-env.sh.template spark-env.sh &&\
    cd /usr/local/spark/conf/ && cp log4j.properties.template log4j.properties &&\
    cd /usr/local/spark/conf/ && sed -i 's/log4j.rootCategory=INFO/log4j.rootCategory=WARN/' log4j.properties &&\
    cd /usr/local/spark/conf/ && echo "SPARK_DIST_CLASSPATH=$(hadoop classpath)" >> spark-env.sh &&\
    cd /usr/local/spark/conf/ && echo "SPARK_MASTER_IP=\$(ifconfig -a | awk '/inet addr/ {print substr(\$2,6)}'| grep -v "127.0.0.1")" >> spark-env.sh


# Spark Master Ports
# masters default web-UI
EXPOSE 8080  
# port to let slaves connect
EXPOSE 7077  
# Rest server port to submit applications
EXPOSE 6066  


# Spark Worker Ports
# Worker web-UI
EXPOSE 8081 
# selecting a random port for the worker, so i can expose it
ENV SPARK_WORKER_PORT 40245  
EXPOSE 40245                 


ENTRYPOINT ["/bin/bash"]
